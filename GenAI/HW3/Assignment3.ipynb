{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMQyEqiJ8PALN9LiDQf84G6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rtniewi/kcwiertniewicz-IDS/blob/main/Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "//***Katrina Cwiertniewicz\n",
        "//*** CSC 330\n",
        "//10/--/2024\n",
        "//Assignment 3: VAE on the SVHN Dataset\n",
        "####The purpose of this assignment is to create and train a Variational Autoencoder model in Keras to learn representations of the Street View House Numbers dataset and explore its performance with different latent dimensions."
      ],
      "metadata": {
        "id": "sMUD1Q77lSBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import(\n",
        "    layers,\n",
        "    models,\n",
        "    datasets,\n",
        "    callbacks,\n",
        "    losses,\n",
        "    optimizers,\n",
        "    metrics,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "IfARqXy8qQ4Y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Load and Preprocess the SVHN Dataset"
      ],
      "metadata": {
        "id": "29s7I1YJmbMR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB_8-hCulIDj",
        "outputId": "7a15ed86-7148-4e8d-a62b-95bb56f4ce72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-04 18:36:27--  http://ufldl.stanford.edu/housenumbers/train_32x32.mat\n",
            "Resolving ufldl.stanford.edu (ufldl.stanford.edu)... 171.64.68.10\n",
            "Connecting to ufldl.stanford.edu (ufldl.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 182040794 (174M) [text/plain]\n",
            "Saving to: ‘train_32x32.mat’\n",
            "\n",
            "train_32x32.mat     100%[===================>] 173.61M  11.8MB/s    in 29s     \n",
            "\n",
            "2024-10-04 18:36:56 (5.97 MB/s) - ‘train_32x32.mat’ saved [182040794/182040794]\n",
            "\n",
            "--2024-10-04 18:36:56--  http://ufldl.stanford.edu/housenumbers/test_32x32.mat\n",
            "Resolving ufldl.stanford.edu (ufldl.stanford.edu)... 171.64.68.10\n",
            "Connecting to ufldl.stanford.edu (ufldl.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64275384 (61M) [text/plain]\n",
            "Saving to: ‘test_32x32.mat’\n",
            "\n",
            "test_32x32.mat      100%[===================>]  61.30M  6.25MB/s    in 8.5s    \n",
            "\n",
            "2024-10-04 18:37:05 (7.20 MB/s) - ‘test_32x32.mat’ saved [64275384/64275384]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://ufldl.stanford.edu/housenumbers/train_32x32.mat\n",
        "!wget http://ufldl.stanford.edu/housenumbers/test_32x32.mat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_data = loadmat('train_32x32.mat')\n",
        "test_data = loadmat('test_32x32.mat')\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = np.transpose(train_data['X'], (3, 0, 1, 2)).astype('float32') / 255.0\n",
        "y_train = train_data['y'].flatten()\n",
        "x_test = np.transpose(test_data['X'], (3, 0, 1, 2)).astype('float32') / 255.0\n",
        "y_test = test_data['y'].flatten()\n",
        "\n",
        "# Display the shape of the datasets\n",
        "print(f'Training data shape: {x_train.shape}')\n",
        "print(f'Test data shape: {x_test.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTAModOLmYie",
        "outputId": "8a7e6ca5-64f3-4bce-f692-e053499733dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (73257, 32, 32, 3)\n",
            "Test data shape: (26032, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Build a Variational Autoencoder"
      ],
      "metadata": {
        "id": "OweiQQtWnzpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampling(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = K.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "metadata": {
        "id": "SZQzrJc3uHfs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vae(latent_dim):\n",
        "  # Encoder\n",
        "  encoder_input = layers.Input(\n",
        "      shape=(32, 32, 3), name=\"encoder_input\"\n",
        "  )\n",
        "  x = layers.Conv2D(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(\n",
        "      encoder_input\n",
        "  )\n",
        "  x = layers.Conv2D(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "  x = layers.Conv2D(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "  shape_before_flattening = K.int_shape(x)[1:]  # the decoder will need this!\n",
        "\n",
        "  x = layers.Flatten()(x)\n",
        "  z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "  z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "  z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "  encoder = models.Model(encoder_input, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "  encoder.summary()\n",
        "\n",
        "  # Decoder\n",
        "\n",
        "  decoder_input = layers.Input(shape=(latent_dim,), name=\"decoder_input\")\n",
        "  x = layers.Dense(np.prod(shape_before_flattening))(decoder_input)\n",
        "  x = layers.Reshape(shape_before_flattening)(x)\n",
        "  x = layers.Conv2DTranspose(\n",
        "      128, (3, 3), strides=2, activation=\"relu\", padding=\"same\"\n",
        "  )(x)\n",
        "  x = layers.Conv2DTranspose(\n",
        "      64, (3, 3), strides=2, activation=\"relu\", padding=\"same\"\n",
        "  )(x)\n",
        "  x = layers.Conv2DTranspose(\n",
        "      32, (3, 3), strides=2, activation=\"relu\", padding=\"same\"\n",
        "  )(x)\n",
        "  decoder_output = layers.Conv2D(\n",
        "      1,\n",
        "      (3, 3),\n",
        "      strides=1,\n",
        "      activation=\"sigmoid\",\n",
        "      padding=\"same\",\n",
        "      name=\"decoder_output\",\n",
        "  )(x)\n",
        "\n",
        "  decoder = models.Model(decoder_input, decoder_output)\n",
        "  decoder.summary()\n",
        "\n",
        "  # Create a variational autoencoder\n",
        "  vae = VAE(encoder, decoder)\n",
        "  return vae"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yeVfyIpt04HE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(models.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Call the model on a particular input.\"\"\"\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        reconstruction = self.decoder(z)\n",
        "        return z_mean, z_log_var, reconstruction\n",
        "\n",
        "    def train_step(self, data):\n",
        "        \"\"\"Step run during training.\"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, reconstruction = self(data)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                BETA\n",
        "                * losses.binary_crossentropy(\n",
        "                    data, reconstruction, axis=(1, 2, 3)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    -0.5\n",
        "                    * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)),\n",
        "                    axis=1,\n",
        "                )\n",
        "            )\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        \"\"\"Step run during validation.\"\"\"\n",
        "        if isinstance(data, tuple):\n",
        "            data = data[0]\n",
        "\n",
        "        z_mean, z_log_var, reconstruction = self(data)\n",
        "        reconstruction_loss = tf.reduce_mean(\n",
        "            BETA\n",
        "            * losses.binary_crossentropy(data, reconstruction, axis=(1, 2, 3))\n",
        "        )\n",
        "        kl_loss = tf.reduce_mean(\n",
        "            tf.reduce_sum(\n",
        "                -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)),\n",
        "                axis=1,\n",
        "            )\n",
        "        )\n",
        "        total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "        return {\n",
        "            \"loss\": total_loss,\n",
        "            \"reconstruction_loss\": reconstruction_loss,\n",
        "            \"kl_loss\": kl_loss,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "Is-d2k0n4Mt6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Train the VAE with different Latent Dimensions"
      ],
      "metadata": {
        "id": "OyUhP8qUsL11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  latent_dims = [2, 5, 10]\n",
        "  for dim in latent_dims:\n",
        "      print(f'Training VAE with latent dimension: {dim}')\n",
        "      vae = build_vae(latent_dim=dim)\n",
        "      vae.compile(optimizer='adam', loss=losses.binary_crossentropy)\n",
        "      vae.fit(x_train, x_train, epochs=1, batch_size=1, validation_data=(x_test, x_test))\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JCfncvrlAF_Q",
        "outputId": "d75d867c-6f2f-4578-812c-df2584347723"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training VAE with latent dimension: 2\n",
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " encoder_input (InputLayer)  [(None, 32, 32, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 32)           896       ['encoder_input[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 8, 8, 64)             18496     ['conv2d_3[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, 4, 4, 128)            73856     ['conv2d_4[0][0]']            \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)         (None, 2048)                 0         ['conv2d_5[0][0]']            \n",
            "                                                                                                  \n",
            " z_mean (Dense)              (None, 2)                    4098      ['flatten_1[0][0]']           \n",
            "                                                                                                  \n",
            " z_log_var (Dense)           (None, 2)                    4098      ['flatten_1[0][0]']           \n",
            "                                                                                                  \n",
            " sampling_1 (Sampling)       (None, 2)                    0         ['z_mean[0][0]',              \n",
            "                                                                     'z_log_var[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 101444 (396.27 KB)\n",
            "Trainable params: 101444 (396.27 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " decoder_input (InputLayer)  [(None, 2)]               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2048)              6144      \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_3 (Conv2D  (None, 8, 8, 128)         147584    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " conv2d_transpose_4 (Conv2D  (None, 16, 16, 64)        73792     \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " conv2d_transpose_5 (Conv2D  (None, 32, 32, 32)        18464     \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " decoder_output (Conv2D)     (None, 32, 32, 1)         289       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 246273 (962.00 KB)\n",
            "Trainable params: 246273 (962.00 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"<ipython-input-8-73b1b35f5c9d>\", line 29, in train_step\n        z_mean, z_log_var, reconstruction = self(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file38fbahmm.py\", line 11, in tf__call\n        (z_mean, z_log_var, z) = ag__.converted_call(ag__.ld(self).encoder, (ag__.ld(inputs),), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'vae_1' (type VAE).\n    \n    in user code:\n    \n        File \"<ipython-input-8-73b1b35f5c9d>\", line 22, in call  *\n            z_mean, z_log_var, z = self.encoder(inputs)\n        File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 219, in assert_input_compatibility\n            raise ValueError(\n    \n        ValueError: Layer \"encoder\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(1, 32, 32, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(1, 32, 32, 3) dtype=float32>]\n    \n    \n    Call arguments received by layer 'vae_1' (type VAE):\n      • inputs=('tf.Tensor(shape=(1, 32, 32, 3), dtype=float32)', 'tf.Tensor(shape=(1, 32, 32, 3), dtype=float32)')\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-a08589a7b654>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-a08589a7b654>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_crossentropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-73b1b35f5c9d>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;34m\"\"\"Step run during training.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstruction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             reconstruction_loss = tf.reduce_mean(\n\u001b[1;32m     31\u001b[0m                 \u001b[0mBETA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_file38fbahmm.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mreconstruction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"<ipython-input-8-73b1b35f5c9d>\", line 29, in train_step\n        z_mean, z_log_var, reconstruction = self(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file38fbahmm.py\", line 11, in tf__call\n        (z_mean, z_log_var, z) = ag__.converted_call(ag__.ld(self).encoder, (ag__.ld(inputs),), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'vae_1' (type VAE).\n    \n    in user code:\n    \n        File \"<ipython-input-8-73b1b35f5c9d>\", line 22, in call  *\n            z_mean, z_log_var, z = self.encoder(inputs)\n        File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 219, in assert_input_compatibility\n            raise ValueError(\n    \n        ValueError: Layer \"encoder\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(1, 32, 32, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(1, 32, 32, 3) dtype=float32>]\n    \n    \n    Call arguments received by layer 'vae_1' (type VAE):\n      • inputs=('tf.Tensor(shape=(1, 32, 32, 3), dtype=float32)', 'tf.Tensor(shape=(1, 32, 32, 3), dtype=float32)')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Reconstruct using the variational autoencoder <a name=\"reconstruct\"></a>"
      ],
      "metadata": {
        "id": "LhYXubGP842C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a subset of the test set\n",
        "n_to_predict = 5000\n",
        "example_images = x_test[:n_to_predict]\n",
        "example_labels = y_test[:n_to_predict]"
      ],
      "metadata": {
        "id": "QjdegoDO8273"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create autoencoder predictions and display\n",
        "z_mean, z_log_var, reconstructions = vae.predict(example_images)\n",
        "print(\"Example _______\")\n",
        "display(example_images)\n",
        "print(\"Reconstructions\")\n",
        "display(reconstructions)"
      ],
      "metadata": {
        "id": "n7bRgiIu9B9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Embed using the encoder <a name=\"encode\"></a>"
      ],
      "metadata": {
        "id": "QBuxFr7m9PcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the example images\n",
        "z_mean, z_var, z = encoder.predict(example_images)"
      ],
      "metadata": {
        "id": "zh628T589SQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some examples of the embeddings\n",
        "print(z[:10])"
      ],
      "metadata": {
        "id": "vfzJxyoS9ULm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the encoded points in 2D space\n",
        "figsize = 8\n",
        "\n",
        "plt.figure(figsize=(figsize, figsize))\n",
        "plt.scatter(z[:, 0], z[:, 1], c=\"black\", alpha=0.5, s=3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FCRHXxC-9Vj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Generate using the decoder <a name=\"decode\"></a>"
      ],
      "metadata": {
        "id": "8ghWGki-9bjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample some points in the latent space, from the standard normal distribution\n",
        "grid_width, grid_height = (6, 3)\n",
        "z_sample = np.random.normal(size=(grid_width * grid_height, 2))"
      ],
      "metadata": {
        "id": "cebZ0Vfs9opq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the sampled points\n",
        "reconstructions = decoder.predict(z_sample)"
      ],
      "metadata": {
        "id": "Co-Y7UNd9mRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert original embeddings and sampled embeddings to p-values\n",
        "p = norm.cdf(z)\n",
        "p_sample = norm.cdf(z_sample)"
      ],
      "metadata": {
        "id": "9vlQnJ3t9i03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw a plot of...\n",
        "figsize = 8\n",
        "plt.figure(figsize=(figsize, figsize))\n",
        "\n",
        "# ... the original embeddings ...\n",
        "plt.scatter(z[:, 0], z[:, 1], c=\"black\", alpha=0.5, s=2)\n",
        "\n",
        "# ... and the newly generated points in the latent space\n",
        "plt.scatter(z_sample[:, 0], z_sample[:, 1], c=\"#00B0F0\", alpha=1, s=40)\n",
        "plt.show()\n",
        "\n",
        "# Add underneath a grid of the decoded images\n",
        "fig = plt.figure(figsize=(figsize, grid_height * 2))\n",
        "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "\n",
        "for i in range(grid_width * grid_height):\n",
        "    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n",
        "    ax.axis(\"off\")\n",
        "    ax.text(\n",
        "        0.5,\n",
        "        -0.35,\n",
        "        str(np.round(z_sample[i, :], 1)),\n",
        "        fontsize=10,\n",
        "        ha=\"center\",\n",
        "        transform=ax.transAxes,\n",
        "    )\n",
        "    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")"
      ],
      "metadata": {
        "id": "2nEJjqo-9bQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 6. Explore the latent space <a name=\"explore\"></a>"
      ],
      "metadata": {
        "id": "HoJO9Jce9rjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colour the embeddings by their label (clothing type - see table)\n",
        "figsize = 8\n",
        "fig = plt.figure(figsize=(figsize * 2, figsize))\n",
        "ax = fig.add_subplot(1, 2, 1)\n",
        "plot_1 = ax.scatter(\n",
        "    z[:, 0], z[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=3\n",
        ")\n",
        "plt.colorbar(plot_1)\n",
        "ax = fig.add_subplot(1, 2, 2)\n",
        "plot_2 = ax.scatter(\n",
        "    p[:, 0], p[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=3\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2ed7o0nE9t1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colour the embeddings by their label (clothing type - see table)\n",
        "figsize = 12\n",
        "grid_size = 15\n",
        "plt.figure(figsize=(figsize, figsize))\n",
        "plt.scatter(\n",
        "    p[:, 0], p[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=300\n",
        ")\n",
        "plt.colorbar()\n",
        "\n",
        "x = norm.ppf(np.linspace(0, 1, grid_size))\n",
        "y = norm.ppf(np.linspace(1, 0, grid_size))\n",
        "xv, yv = np.meshgrid(x, y)\n",
        "xv = xv.flatten()\n",
        "yv = yv.flatten()\n",
        "grid = np.array(list(zip(xv, yv)))\n",
        "\n",
        "reconstructions = decoder.predict(grid)\n",
        "# plt.scatter(grid[:, 0], grid[:, 1], c=\"black\", alpha=1, s=10)\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure(figsize=(figsize, figsize))\n",
        "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "for i in range(grid_size**2):\n",
        "    ax = fig.add_subplot(grid_size, grid_size, i + 1)\n",
        "    ax.axis(\"off\")\n",
        "    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")"
      ],
      "metadata": {
        "id": "P6aD9mKy92G7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
